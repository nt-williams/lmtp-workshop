---
title: "Estimators"
bibliography: references.bib
nocite: |
  @diaz2023nonparametric, @hoffman2023introducing, @kennedy2019nonparametric @haneuse2013estimation, @bickel1993efficient
---

{{< include macros.qmd >}}

To recap, we've

1.  defined a causal parameter $\theta$ that is a function of a general hypothetical intervention, $\dd_t(a_t, h_t, \epsilon)$, and

2.  defined the necessary assumptions to identify the expected value of $Y$ when $A$ is replaced with the output of $\dd_t(a_t, h_t, \epsilon)$ from observed data.

It's now time to discussing how to estimate these parameters.

### Sequential Regression Estimator

One possible estimator is simply a *plug-in* estimator of the identification result. This estimator is often referred to as G-computation.

::: algorithm
::: algorithm-header
Algorithm: Sequential regressions
:::

::: algorithm-body
1.  Set $\m_{i,\tau +1} = Y_i$.

    For $t = \tau, ..., 1$:

    a.  Using a pre-specified parametric model, regress $\m_{i,t+1}$ on $\{A_{i, t}, H_{i,t}\}$.

    b.  Generate predictions from this model with $A_{i,t}$ changed to $A^{\dd}_{i,t}$. Set $\m_{i, t}$ to be these predicted values.

    c.  Repeat (iterate) the above two steps until setting $\m_{i, 1}$ to the predicted values.

2.  Take the final estimate as $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n \hat\m_{i, 1}$.

3.  Compute standard errors using a bootstrap of steps 1 and 2.
:::
:::

+----------------------------+----------------------------------------------------+
| Pros ‚úÖ                    | Cons ‚ùå                                            |
+:==========================:+:==================================================:+
| -   Simple to implement    | -   Requires correct estimation of all regressions |
|                            |                                                    |
| -   Substitution estimator | -   Requires pre-specified parametric models       |
+----------------------------+----------------------------------------------------+

: {tbl-colwidths="\[25,29\]"}

A substitution estimator is nice, because its estimates are guaranteed to stay within the valid range of the outcome.

### Density-ratio estimation {#sec-density-ratio-estimation}

::: {.callout-caution appearance="simple"}
The next three estimators all rely on estimating the density ratio

$$
r_t(a_t, h_t) = \frac{\g_t^\dd(a_t \mid h_t)}{\g_t(a_t \mid h_t)}.
$$We will often refer to this ratio as the *intervention mechanism* throughout the workshop. Estimation of $r_t(a_t, h_t)$ is fully automated in `lmtp` and hidden from the user. **A comprehensive understanding of this process isn't necessary to use `lmtp`!**
:::

-   We can directly estimate this density ratio with a classification trick.

-   Can compute the density ratio (which we can see is also the odds) in a classification problem. We use an augmented dataset with $2n$ observations. In this new dataset, the outcome is a new variable that we make, $\Lambda$, (defined below) and the predictors are the variables $A_t$ and $H_t$. In this new dataset, the data structure at time $t$ is redefined as

$$
(H_{\lambda, i, t}, A_{\lambda, i, t}, \Lambda_{\lambda, i} : \lambda = 0, 1; i = 1, ..., n)
$$

-   $\Lambda_{\lambda, i} = \lambda_i$ indexes duplicate values. So if $\Lambda_i =1$ if observation $i$ is a duplicated value and $\Lambda_i =0$ otherwise.

    -   For all duplicated observations $\lambda\in\{0,1\}$ with the same $i$, $H_{\lambda, i, t}$ is the same

    -   For all the non-duplicated observations, $\lambda = 0$, $A_{\lambda=0, i, t}$ equals the observed exposure values $A_{i, t}$

    -   For all the duplicated observations, $\lambda=1$, $A_{\lambda=1, i, t}$ equals the exposure values under the intervention $\dd$, $A^{\dd}_{i,t}$

-   We then estimate the conditional probability that $\Lambda=1$ in this dataset, and divide it by the corresponding estimate of the conditional probability that $\Lambda=0$. Specifically, denoting $p^\lambda$ to be the distribution of the data in the augmented dataset, we have:

$$
r_t(a_t, h_t) = \frac{p^\lambda(a_t, h_t \mid \Lambda =
    1)}{p^\lambda(a_t, h_t \mid \Lambda =
    0)}=\frac{p^\lambda(\Lambda = 1\mid A_t=a_t,
    H_t=h_t)}{p^\lambda(\Lambda = 0\mid A_t=a_t, H_t=h_t)}
$$

### Inverse Probability Weighting

$$
\theta = \E \bigg[ \bigg\{\prod_{t=1}^\tau r_t(a_t, h_t) \bigg\} Y \bigg]
$$

::: algorithm
::: algorithm-header
Algorithm: IPW Estimator
:::

::: algorithm-body
1.  Construct estimates of $r_{i,t}(a_t, h_t)$ using the density ratio classification trick and a pre-specified parametric model.

2.  Define the weights $w_{i} = \prod_{t=1}^\tau r_{i,t}(a_t, h_t)$.

3.  Take the final estimate as $\hat{\theta} = \frac{1}{n}\sum_{i=1}^n \hat{w}_{i}\times y_i$.

4.  Compute standard errors using a bootstrap of steps 1-3.
:::
:::

+-------------------------+---------------------------------------------------+
| Pros ‚úÖ                 | Cons ‚ùå                                           |
+:=======================:+:=================================================:+
| -   Simple to implement | -   Requires correct estimation of density ratios |
|                         |                                                   |
|                         | -   Requires pre-specified parametric models      |
+-------------------------+---------------------------------------------------+

: {tbl-colwidths="\[25,25\]"}

### Doubly Robust Estimators

G-computation and IPW estimators require the estimation of nuisance parameters with correctly specified parametric models. We will now turn our attention to two non-parametric estimators:

1.  targeted minimum-loss based estimator (TMLE), and

2.  a sequentially doubly-robust estimator (SDR).

Wait, what does it mean for an estimator to be doubly robust?

-   For the simple case of a single time point, an estimator is considered doubly robust if it is able to produce a consistent estimate of the target parameter as long as either the outcome model is consistently estimated or the treatment (and censoring) model(s) are consistently estimated.

-   For time-varying setting, an estimator is doubly robust if, for some time $s$, all outcome regressions for $t >s$ are consistently estimated and all intervention mechanisms (treatment + censoring) for $t \leq s$ are consistently estimated.

-   Sequential double robustness (often also referred to as $2^\tau$-multiply robust) implies that an estimator is consistent if for all times either the outcome or intervention mechanism (treatment + censoring) is consistently estimated.

#### Efficient Influence Function

Key to constructing the TMLE and SDR is the *efficient influence function* (EIF).

-   The EIF characterizes the asymptotic behavior of all regular and efficient estimators.

-   The EIF characterizes the first-order bias of pathwise differentiable estimands.

Before we introduce the EIF, it's necessary to make some additional assumptions on $A$ and $\dd$.

::: callout-important
## Assumptions

1.  The treatment $A$ is discrete, or

2.  If $A$ is continuous, the function $\dd$ is piecewise smooth invertible

3.  The function $\dd$ does not depend on the observed distribution $\P$
:::

-   These assumptions ensure that the efficient influence function of $\theta$ for interventions $\dd$ have a structure similar to the influence function for the effect of dynamic regimes.

-   This allows for multiply robust estimation, which is not generally possible for interventions $\dd$ that depend on $\P$.

Define the function

$$
\phi_t: O \mapsto \sum_{s=t}^\tau \bigg( \prod_{k=t}^s r_k(a_k, h_k)\bigg) \big\{\m_{s+1}(a_{s+1}^\dd, h_{s+1}) - \m_s(a_s, h_s) \big\} + \m_t(a_t^\dd, h_t).
$$

The efficient influence function for estimating $\theta = \E[\m_1(A^\dd, L_1)]$ in the non-parametric model is given by $\phi_1(O) - \theta$. In the case of single time-point, the influence function simplifies to

$$
r(a, w)\{Y - \m(a,w)\} + \m(a^{\dd},w) - \theta.
$$

#### Targeted Minimum-Loss Based Estimation

::: algorithm
::: algorithm-header
Algorithm: TMLE
:::

::: algorithm-body
1.  Construct estimates of $r_{i,t}(a_t, h_t)$ using the density ratio classification trick and your favorite regression method.

2.  For $t = 1, ..., \tau$, compute the weights: $w_{i,t} = \prod_{k=1}^t r_{i,k}(a_{i,k}, h_{i,k})$

3.  Set $\tilde{\m}_{i,\tau +1}(A^\dd_{i,t+1}, H_{i,t+1}) = Y_i$.

    For $t = \tau, ..., 1$:

    1.  Regress $\tilde{\m}_{i,t+1}(A^\dd_{i,t+1}, H_{i,t+1})$ on $\{A_{i, t}, H_{i,t}\}$.

        -   Using this regression, generate predictions from $\{A_{i, t}, H_{i,t}\}$ and $\{A^\dd_{i, t}, H_{i,t}\}$.

        -   Denote the predictions as $\tilde{\m}_t(A_{i,t}, H_{i,t})$ and $\tilde{\m}_t(A^\dd_{i,t}, H_{i,t})$ respectively.

    2.  Fit the generalized linear tilting model:

        $\text{link }\tilde{\m}^\epsilon_t(A_{i,t}, H_{i,t}) = \epsilon + \text{link }\tilde{\m}_{i,t}(A_{i,t}, H_{i,t})$

        with weights $w_{i,t}$.

        -   $\text{link }\tilde{\m}_{i,t}(A_{i,t}, H_{i,t})$ is an offset variable (i.e., a variable with known parameter value equal to one).

        -   The parameter $\epsilon$ may be estimated by running a generalized linear model of $\tilde{\m}_{i,t+1}(A^\dd_{i,t+1}, H_{i,t+1})$ with only an intercept term, an offset term equal to $\text{link }\tilde{\m}_{i,t}(A_{i,t}, H_{i,t})$, and weights $w_{i,t}$.

    3.  Let $\hat\epsilon$ be the maximum likelihood estimate, and update the estimates as:

        $\text{link }\tilde{\m}^\hat\epsilon_t(A^\dd_{i,t}, H_{i,t}) = \hat\epsilon + \text{link }\tilde{\m}_t(A^\dd_{i,t}, H_{i,t})$

        $\text{link }\tilde{\m}^\hat\epsilon_t(A_{i,t}, H_{i,t}) = \hat\epsilon + \text{link }\tilde{\m}_t(A_{i,t}, H_{i,t})$

    4.  Update $\tilde{\m}_{i,t} = \tilde{\m}^\hat\epsilon_{i,t}$, $t = t-1$, and iterate.

4.  The final estimate is defined as $\hat\theta = \frac{1}{n}\sum_{i=1}^n\tilde{m}_{i, 1}(A^\dd_{i, 1}, L_{i, 1})$.
:::
:::

+------------------------------+------------------------------------+
| Pros ‚úÖ                      | Cons ‚ùå                            |
+:============================:+:==================================:+
| -   Substitution estimator   | -   not sequentially doubly-robust |
|                              |                                    |
| -   doubly-robust            |                                    |
|                              |                                    |
| -   can use machine learning |                                    |
+------------------------------+------------------------------------+

: {tbl-colwidths="\[25,25\]"}

#### Sequentially Doubly Robust Estimator

::: algorithm
::: algorithm-header
Algorithm: SDR Estimator
:::

::: algorithm-body
1.  Construct estimates of $r_{i,t}(a_t, h_t)$ using the density ratio classification trick and your favorite regression method.

2.  Initialize $\phi_{\tau +1}(O_i) = Y_i$.

    For $t = \tau, ..., 1$:

    1.  Compute the pseudo-outcome $\check{Y}_{i,t+1} = \phi_{t+1}(O_i)$.

    2.  Regress $\check{Y}_{i,t+1}$ on $\{A_{i, t}, H_{i,t}\}$.

    3.  Let $\check\m_{i,t}$ denote the predicted values. Update $\m_{i,t} = \check\m_{i,t}$ and iterate.

3.  The final estimate is defined as $\hat\theta = \frac{1}{n}\sum_{i=1}^n\phi_1(O_i).$
:::
:::

+--------------------------------+----------------------------------+
| Pros ‚úÖ                        | Cons ‚ùå                          |
+:==============================:+:================================:+
| -   doubly-robust              | -   not a substitution estimator |
|                                |                                  |
| -   sequentially doubly-robust |                                  |
|                                |                                  |
| -   can use machine learning   |                                  |
+--------------------------------+----------------------------------+

: {tbl-colwidths="\[25,25\]"}

### Choosing an Estimator

-   In general we never recommend using the IPW or sequential regression estimator. Both require the use of correctly pre-specified parametric models for valid statistical inference (ya right üòÇ).

-   The TMLE and SDR estimators, however, are both doubly or sequentially doubly robust and can be used with machine-learning algorithms while remaining $\sqrt{n}$-consistent.

|                                       | IPW | G-comp. | TMLE | SDR |
|---------------------------------------|:---:|:-------:|:----:|:---:|
| Uses outcome regression               |     |   ‚≠ê    |  ‚≠ê  | ‚≠ê  |
| Uses the propensity score             | ‚≠ê  |         |  ‚≠ê  | ‚≠ê  |
| Valid inference with machine-learning |     |         |  ‚≠ê  | ‚≠ê  |
| Substitution estimator                |     |   ‚≠ê    |  ‚≠ê  |     |
| Doubly robust                         |     |         |  ‚≠ê  | ‚≠ê  |
| Sequentially doubly robust            |     |         |      | ‚≠ê  |

: **Table 1.** Summary of estimator properties. {tbl-colwidths="\[40,15,15,15,15\]"}

::: {.callout-warning icon="false"}
## Question

Why can't we use machine learning with the G-computation and IPW estimators?

<details>

<summary style="color: black; font-weight: bold;">

Answer

</summary>

The G-computation and IPW estimators require models that converge to the truth at a $\sqrt{n}$-rate. Machine learning algorithms are not guaranteed to do this.

</details>
:::

::: callout-tip
## Which estimator should I use?

While the SDR estimator may be more robust to model misspecification, the TMLE does have the advantage of being a substitution estimator. Because of this, estimates from the TMLE are guaranteed to stay within the valid range of the outcome. Taken together, this leads to the following recommendations for choosing between the TMLE and SDR:

-   If treatment is not time-varying, use the TMLE.

-   If treatment is time-varying first try the SDR.

-   If the SDR estimator produces "out-of-bounds" estimates, instead use the TMLE.
:::

### Cross-fitting

When estimating nuisance parameters with data adaptive algorithms, you should perform a process similar to cross-validation called cross-fitting. Cross-fitting helps ensure:

-   that standard errors will be correct, and

-   can help reduce estimator bias.

Cross-fitting is fully automated in `lmtp`, but for more information we recommend reviewing @chernozhukov2018double, @diaz2020machine, and @zivich2021machine.
