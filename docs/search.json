[
  {
    "objectID": "info_d.html",
    "href": "info_d.html",
    "title": "Defining General, Hypothetical Interventions",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]\nHave you ever begun reading a paper in the methodological causal inference literature and encountered the phrase “assume the treatment or exposure is a binary…”? (Most papers we read assume this!!) While assuming exposure variables are binary can simplify the definition of causal effects, many exposures of interest in reality are not binary.\nInstead, we will work in situations where \\(A\\) is a binary, categorical, multivariate, or continuous variable!\nIn the previous section, we defined \\(\\dd(a_t, h_t, \\epsilon_t)\\) as a function that maps values \\(a_t\\), \\(h_t\\), and potentially a randomizer \\(\\epsilon_t\\) to a new value of treatment. Let us see how this function is used to define a treatment effect. Our focus henceforth is estimating the causal effect of an intervention, characterized by \\(\\dd\\) on the outcome \\(Y\\), through the causal parameter\n\\[\n\\theta = \\E[Y^{\\bar A^{\\dd}}]\\text{,}\n\\]\nwhere \\(Y^{\\bar A^{\\dd}}\\) is the counterfactual outcome in a world, where possibly contrary to fact, each entry of \\(\\bar{A} = (A_1, \\ldots, A_\\tau)\\) was modified according to the function \\(\\dd\\) as follows.\nThis definition is agnostic to the type of outcome:\nBut what is this function \\(\\dd\\), how can it be defined, and how does using this function to define interventions solve the problem? Let’s start from simple to more complex examples of functions \\(\\dd\\).",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#static-interventions",
    "href": "info_d.html#static-interventions",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Static Interventions",
    "text": "Static Interventions\nLet \\(A\\) denote a binary vector, such as receiving a medication, and define \\(\\dd(a_t, h_t, \\epsilon) = 1\\). This intervention characterizes a hypothetical world where all members of the population receive treatment at all times.\n\n\n\n\n\n\nAn intervention is static if the function \\(\\dd_t\\) always returns the same value regardless of the input.\n\n\n\n\nExample\n\nLet’s say we were interested in the effect of randomizing patients with opioid use disorder to injection naltrexone (\\(A=1\\)) vs. sublingual buprenorphine (\\(A=0\\)). We would contrast the counterfactual outcomes in a hypothetical world in which all units were treated with injection naltrexone \\(\\dd_1=1\\) versus a hypothetical world in which all units were treated with buprenorphine \\(\\dd_0=0\\). This gives us the well-known average (comparative) treatment effect (ATE). \\[\\E[Y^{\\bar A^{\\dd_1}} - Y^{\\bar A^{\\dd_0}}] = \\E[Y^{A=1} - Y^{A=0}]\\]",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#dynamic-treatment-regime",
    "href": "info_d.html#dynamic-treatment-regime",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Dynamic Treatment Regime",
    "text": "Dynamic Treatment Regime\nLet \\(A_t\\) denote a binary vector, such as receiving a medication, and \\(H_t\\) a numeric vector, such as a measure of discomfort. For a given value of \\(\\delta\\), define \\[\n\\dd(a_t, h_t, \\epsilon) = \\begin{cases}\n1 &\\text{ if } h_t &gt; \\delta \\\\\n0 &\\text{ otherwise.}\n\\end{cases}\n\\]\n\n\n\n\n\n\nInterventions where the output of the function \\(\\dd\\) depends only on the covariates \\(H_t\\) are referred to as being dynamic.\n\n\n\n\nExample\n\nRudolph, Williams, et al. (2022) examined a Buprenorphine (BUP-NX) dosing strategy among a population of patients who were taking BUP-NX for opioid use disorder. Under the hypothetical intervention, patients who reported opioid use during the week prior to a physicians exam received a BUP-NX dose increase while patients who did not report prior-week opioid use maintained the same dose. Let \\(A_t\\) be a binary indicator for BUP-NX dose increase at week \\(t\\) compared to week \\(t-1\\) and \\(X_t\\) be an indicator for opioid use at week \\(t\\). Then, \\[\n\\dd(a_t, h_t, \\epsilon) = \\begin{cases}\n1 \\text{ if } x_{t-1} = 1\\\\\n0 \\text{ otherwise}\n\\end{cases}\n\\]",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#modified-treatment-policies",
    "href": "info_d.html#modified-treatment-policies",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Modified Treatment Policies",
    "text": "Modified Treatment Policies\nWhile much attention is given to static and dynamic interventions, their use is often accompanied by a few key problems.\n\nDefining causal effects in terms of hypothetical interventions where treatment is applied to all units may be inconceivable. For example, we may be interested to know if reducing surgery time reduces surgical complications. However, it’s inconceivable to set all surgeries to a given duration, even if this duration depends on patient covariates.\nDefining causal effects in terms of hypothetical interventions where treatment is applied to all units may induce positivity violations.\n\nA solution to these problems is to instead define causal effects using modified treatment policies (MTP).\n\n\n\n\n\n\nAn intervention characterized by a hypothetical world where the natural value of treatment is modified is called a modified treatment policy.\n\n\n\n\nAdditive and multiplicative shift MTP\nLet \\(A_t\\) denote a numeric vector. Assume that \\(A_t\\) has support in the data such that \\(P(A_t \\leq u(h_t) \\mid H_t = h_t) = 1\\). For an analyst-defined value of \\(\\delta\\), define \\[\n\\dd(a_t, h_t, \\epsilon) = \\begin{cases}\na_t + \\delta &\\text{ if } a_t + \\delta \\leq u(h_t) \\\\\na_t &\\text{ otherwise.}\n\\end{cases}\n\\]\nUnder this intervention, the natural value of exposure at time \\(t\\) is increased by the analyst-defined value \\(\\delta\\), whenever such an increase is feasible. This MTP is referred to as an additive shift MTP.\n\nExample\n\nDı́az et al. (2023) estimated the effect of increasing P/F ratio (a measure of hypoxemia) by 50 units on survival among those patients with acute respiratory failure (a P/F ratio &lt; 300). \\[\n\\dd_t(a_t, h_t, \\epsilon) = \\begin{cases}\na_t + 50 &\\text{ if } a_t \\leq 300 \\\\\na_t &\\text{ otherwise}\n\\end{cases}\n\\]\n\nWe can similarly define a multiplicative shift MTP as\n\\[\n\\dd(a_t, h_t, \\epsilon) = \\begin{cases}\na_t \\times \\delta &\\text{ if } a_t \\times \\delta \\leq u(h_t) \\\\\na_t &\\text{ otherwise}.\n\\end{cases}\n\\]\n\n\nExample\n\nNugent and Balzer (2023) evaluated the association between county-level measures of mobility and incident COVID-19 cases in the United States in the Summer and Fall of 2022. They considered both hypothetical additive and multiplicative MTPs; for example, they defined a multiplicative MTP where a measure for the density of mobile devices visiting commercial locations was decreased by 25%: \\[\n\\dd(a_t, h_t, \\epsilon) = a_t \\times 0.75.\n\\]",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#randomized-interventions",
    "href": "info_d.html#randomized-interventions",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Randomized Interventions",
    "text": "Randomized Interventions\nLet \\(A\\) denote a binary vector, \\(\\epsilon \\sim U(0, 1)\\), and \\(\\epsilon\\) be an analyst-defined value between 0 and 1. We may then define randomized interventions. For example, imagine we are interested in a hypothetical world where half of all smokers quit smoking. This intervention would be defined as\n\\[\n\\dd(a_t, \\epsilon_t) = \\begin{cases}\n0 &\\text{ if } \\epsilon_t &lt; 0.5 \\text{ and } a_t = 1 \\\\\na_t &\\text{ otherwise}\n\\end{cases}.\n\\]",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#incremental-propensity-score-interventions-based-on-the-risk-ratio",
    "href": "info_d.html#incremental-propensity-score-interventions-based-on-the-risk-ratio",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Incremental Propensity Score Interventions Based on the Risk Ratio",
    "text": "Incremental Propensity Score Interventions Based on the Risk Ratio\nLet \\(A\\) denote a binary variable, \\(\\epsilon \\sim U(0, 1)\\), and \\(\\delta\\) be an analyst-defined risk ratio limited to be between \\(0\\) and \\(1\\). In addition, define \\(P(A_t = a_t\\mid H_t)= \\g(a_t \\mid H_t)\\).\nIf we were interested in an intervention that decreased the likelihood of receiving treatment, define\n\\[\n\\dd_t(a_t, h_t, \\epsilon_t) = \\begin{cases}\na_t &\\text{ if } \\epsilon_t &lt; \\delta \\\\\n0 &\\text{ otherwise}\n\\end{cases}.\n\\] In this case, we have \\(\\g^\\dd(a_t \\mid H_t) = a_t \\delta \\g_t(1 \\mid H_t) + (1 - a_t) (1 - \\delta \\g_t(1\\mid H_t))\\), which leads to a risk ratio of \\(\\g_t^\\dd(1 \\mid H_t)/\\g_t(1\\mid H_t) = \\delta\\) for comparing the propensity score post- vs pre-intervention.\n\n\n\n\n\n\nAn intervention where the conditional probability of treatment is shifted is referred to as an incremental propensity score intervention.\n\n\n\nConversely, if we were interested in an intervention that increased the likelihood of receiving treatment, define\n\\[\n\\dd_t(a_t, h_t, \\epsilon_t) = \\begin{cases}\na_t &\\text{ if } \\epsilon_t &lt; \\delta \\\\\n1 &\\text{ otherwise.}\n\\end{cases}\n\\]\nNow \\(\\g_t^\\dd(a_t \\mid H_t) = a_t (1 - \\delta \\g_t(0\\mid H_t)) + (1 - a_t) \\delta \\g_t(0 \\mid H_t)\\), which implies a risk ratio \\(\\g_t^\\dd(0\\mid H_t)/\\g(0\\mid H_t) = \\delta\\).\n\n\n\n\n\n\nInterventions where the shift is in the odds ratio scale were previously proposed, but the effects of odds-ratio shifts should not be estimated with lmtp, we will discuss this more later.\n\n\n\n\nExample\n\nUsing electronic health record data, Wen, Marcus, and Young (2023) estimated the effect of increasing the proportion of PrEP uptake on bacterial STI among cis-gender males being tested for STIs and that do not have HIV. Let \\(A_t\\) be a binary indicator for PrEP initiation at week \\(t\\), and \\(L_t\\) be a binary indicator for any STI testing and being HIV-free at week \\(t\\). They defined a “medium” successful PrEP uptake intervention as \\[\n\\dd(a_t, h_t, \\epsilon) = \\begin{cases}\na_t &\\text{ if } l_t = 1 \\text{ and } \\epsilon_t &lt; 0.85 \\\\\n1 &\\text{ otherwise}.\n\\end{cases}\n\\]",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "info_d.html#identification-of-the-causal-parameter",
    "href": "info_d.html#identification-of-the-causal-parameter",
    "title": "Defining General, Hypothetical Interventions",
    "section": "Identification of the causal parameter",
    "text": "Identification of the causal parameter\nRecall that the fundamental problem of causal inference is that we can’t observe the alternative worlds which we use to define causal effects. If we can’t observe counterfactual variables, then how can we learn a causal effect? Under a set of certain assumptions, we can identify a causal parameter from observed data. These assumptions are called identification assumptions.\n\nIdentification Assumptions\n\nPositivity. If \\((a_t, h_t) \\in \\text{supp}\\{A_t, H_t\\}\\) then \\(\\dd(a_t, h_t) \\in \\text{supp}\\{A_t, H_t\\}\\) for \\(t \\in \\{1, ..., \\tau\\}\\).\n\nIf there is a unit with observed treatment value \\(a_t\\) and covariates \\(h_t\\), there must also be a unit with treatment value \\(\\dd(a_t, h_t)\\) and covariates \\(h_t\\).\n\nNo unmeasured confounders. All the common causes of \\(A_t\\) and \\((L_s, A_s, Y)\\) are measured and contained in \\(H_t\\) for all \\(s \\in \\{t+1, ..., \\tau\\}\\).\n\nFor all times \\(t\\), the history \\(H_t\\) contains sufficient variables to adjust for confounding of \\(A_t\\) and any subsequent variables, including future treatment.\n\n\n\n\n\n\n\n\nQuestion: When might these assumptions be violated?\n\n\n\nAssuming the above, \\(\\theta\\) is identified from the observed data with:\nSet \\(\\m_{\\tau+1} = Y\\). In a slight abuse of notation, let \\(A_t^\\dd = \\dd(A_t, H_t)\\). For \\(t = \\tau, ..., 1\\), recursively define\n\\[\n\\m_t: (a_t, h_t) \\rightarrow \\E[\\m_{t + 1}(A^{\\dd}_{t+1}, H_{t + 1}) \\mid A_t = a_t, H_t = h_t],\n\\]\nand define \\(\\theta = E[\\m_1(A^{\\dd}_1, L_1)]\\).\n\n\nExample\nConsider the following data (here \\(\\tau = 2\\)):\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nand the intervention\n\\[\n\\dd(a_t, \\epsilon_t) = \\begin{cases}\n0 &\\text{ if } \\epsilon_t &lt; 0.5 \\text{ and } a_t = 1 \\\\\na_t &\\text{ otherwise.}\n\\end{cases}\n\\]\nThe true value under this intervention is approximately \\(-0.37\\). First, let’s translate this intervention into an R function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can then compute the identification formula in the following steps:\n\nSet \\(\\m_3(A_3^\\dd, H_3) = Y\\)\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute the regression of \\(\\m_3(A_3^\\dd, H_3)\\) on \\((A_2, H_2)\\). This gives a predictive function, call that predictive function \\(\\m_2(A_2,H_2)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUse the predictive function to compute what would have occurred if the intervention had been implemented at time \\(t=2\\), i.e., compute \\(\\m_2(A_2^\\dd,H_2)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute the regression of \\(\\m_2(A_2^\\dd,H_2)\\) on \\((A_1, H_1)\\). This gives a predictive function, call that predictive function \\(\\m_1(A_1,H_1)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nUse the predictive function to compute what would have occurred if the intervention had been implemented at time \\(t=1\\), i.e., compute \\(\\m_1(A_1^\\dd,H_1)\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nCompute the mean of \\(\\m_1(A_1^\\dd,H_1)\\). This mean is equal to \\(\\theta\\).\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "2. Defining Interventions"
    ]
  },
  {
    "objectID": "R_mtp.html",
    "href": "R_mtp.html",
    "title": "Modified treatment policies",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]\nAs we already discussed, a modified treatment policy is an intervention characterized by a dependence on the natural, or observed, value of treatment. Examples include:",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Modified Treatment Policies"
    ]
  },
  {
    "objectID": "R_mtp.html#problem-1",
    "href": "R_mtp.html#problem-1",
    "title": "Modified treatment policies",
    "section": "Problem 1",
    "text": "Problem 1\nWrite a shift function for a modified treatment policy where the natural value of neighborhood violence is decreased by 50%. Using the lmtp package, estimate the population mean effect of this intervention on preterm birth. Make sure to use the id argument to account for community level clustering in the estimation of standard errors. To save computation time, don’t use cross-fitting.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nd_mtp &lt;- function(data, a) {\n  data[[a]]*0.5\n}\n\nfit_mtp &lt;- lmtp_tmle(\n  data = crime, \n  trt = A, \n  outcome = Y, \n  baseline = W,\n  id = id,\n  shift = d_mtp, \n  mtp = TRUE,\n  folds = 1, \n  learners_trt = learners, \n  learners_outcome = learners\n)\n\n\nDid you get this error?\n\nWarning: Detected decimalish `trt` values and `mtp = FALSE`. Consider setting `mtp =\nTRUE` if getting errors.\nLoading required package: nnls\nError: object 'fit' not found\n\n\n\n\n\n\n\nMake sure to set mtp = TRUE if you’re intervention is a modified treatment policy!",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Modified Treatment Policies"
    ]
  },
  {
    "objectID": "R_mtp.html#problem-2",
    "href": "R_mtp.html#problem-2",
    "title": "Modified treatment policies",
    "section": "Problem 2",
    "text": "Problem 2\nLet’s compare the effect of the intervention on reducing neighborhood violence on preterm birth to the observed preterm birth rate. Use the lmtp_contrast() function.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nlmtp_contrast(fit_mtp, ref = mean(crime$preterm_birth))\n\n\nWith lmtp we found that under a hypothetical intervention that reduces neighborhood violence by 50%, preterm birth would decrease by approximately 4.2 percentage points.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Modified Treatment Policies"
    ]
  },
  {
    "objectID": "R_mtp.html#footnotes",
    "href": "R_mtp.html#footnotes",
    "title": "Modified treatment policies",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThank you to Chris Dharma, who simulated this data!↩︎",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Modified Treatment Policies"
    ]
  },
  {
    "objectID": "R_survival.html",
    "href": "R_survival.html",
    "title": "Survival Analysis",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "R_survival.html#data-considerations",
    "href": "R_survival.html#data-considerations",
    "title": "Survival Analysis",
    "section": "Data considerations",
    "text": "Data considerations\n\nSimilar to when the outcome is binary, survival outcomes should be coded using 0’s and 1’s where 1 indicates the occurrence of an event and 0 otherwise.\nSimilar to how we encode censoring variables, we consider the outcome to be degenerate. Meaning that once an observation experiences an outcome, all future outcome variables should also be coded with a 1 (“last-observation-carried-forward”).",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "R_survival.html#point-treatment-survival-problems",
    "href": "R_survival.html#point-treatment-survival-problems",
    "title": "Survival Analysis",
    "section": "Point-treatment survival problems",
    "text": "Point-treatment survival problems\nUp to this point, we’ve been ignoring that the covid dataset should be treated as a point-treatment survival problem. Let’s re-estimate the effect of the randomized treatment with a survival framework.\n\nWe need to transform the data from long to wide format\nand impute the outcome using last-observation-carried-forward.\n\nOur modified dataset should look like this:\n\n\n\nStructure of data with survival outcome and point-treatment. Adapted from Hoffman et al., 2022.\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nUse the function event_locf() to make sure the outcome variables are correctly recorded.\n\n\n\n\nInstead of just estimating the effect of treatment on the outcome at the last time point, we can estimate the effect of a treatment on an outcome at all follow-up intervals.\n\nLet’s estimate the effect of the treatment on intubation at each day. To do so, we can use the function lmtp_survival() .\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can now visualize our results using a survival plot.\n\n\n\n\n\n\nThe main result of an lmtp object can be extracted using the tidy() function from the broom package.\n\n\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "R_survival.html#time-varying-treatment",
    "href": "R_survival.html#time-varying-treatment",
    "title": "Survival Analysis",
    "section": "Time-varying treatment",
    "text": "Time-varying treatment\nHoffman et al. (2024) demonstrated the use of modified treatment policies for survival outcomes to assess the effect of preventing invasive mechanical ventilation (IMV) on mortality among patients hospitalized with COVID-19 in New York City during the first COVID-19 wave. A synthetic version of the data used for that analysis has been loaded into R as intubation.\n\n\n\nStructure of data with survival outcome and time-varying treatment. Adapted from Hoffman et al., 2022.\n\n\n\nThe data consists of \\(n = 2000\\) observations hospitalized with COVID-19 and who were followed for \\(\\tau = 14\\) days.\nThere are 10 baseline confounders and 4 time-varying confounders.\nThe outcome of interest is an indicator for death on day \\(t\\).\nObservations are subject to loss-to-follow-up due to either hospital discharge or transfer.\n\n\n\n\n\n\n\n\n\nLet’s consider the following intervention\n\\[\n\\dd_t(a_t, h_t) = \\begin{cases}\n1 \\text{ if } a_t = 2\\\\\na_t \\text{ otherwise},\n\\end{cases}\n\\]\nwhere \\(A_t\\) is a 3-level categorical variable: 0, no supplemental oxygen; 1, non-IMV supplemental oxygen support; 2, IMV.\nIn words, this function corresponds to an intervention where patients who were naturally observed as receiving IMV on day \\(t\\) instead receive non-IMV supplemental oxygen. Let’s translate this policy to an R function that we can use with lmtp.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\n\n\n\nWe can now estimate the effect of preventing IMV on 14-day mortality.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "R_survival.html#competing-risks",
    "href": "R_survival.html#competing-risks",
    "title": "Survival Analysis",
    "section": "Competing risks",
    "text": "Competing risks\nIn the context of survival analysis, competing risks refer to events that preclude the occurrence of the primary event of interest. In the previous example, we treated hospital discharge or transfer as censoring events. These events, however, are actually competing risks because we know that if a patients was discharged or transferred out of the ICU on day \\(t\\) they didn’t die on that day.\n\n\n\n\n\n\nTreating competing risks as censoring events involves estimating the effect of an intervention that eliminates the competing event. The identification assumptions for this intervention are stronger than intervention that doesn’t consider eliminating the competing event.\n\n\n\nIn the presence of competing risks, lmtp can estimate cumulative incidence effects. Cumulative incidence effect can be interpreted as the total effect of treatment operating through pathways that include the competing events. Let’s re-evaluate the effect of the previous intervention on 14-day mortality but instead treat discharge or transfer as a competing risk. We first need to modify the data:\n\nFlip the columns corresponding to discharge or transfer so that a 1 indicates a discharge or transfer occurred\nImpute missing values for discharge and death using last-observation carried forward because they are both deterministic variables once they have occurred (i.e., if a patient died, their probability of discharge or transfer is 0 and vice-versa)\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nWe can now re-estimate the effect of the intervention. Instead of passing the discharge vector to the cens argument, we pass it to the compete argument.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Survival Analysis"
    ]
  },
  {
    "objectID": "info_estimators.html",
    "href": "info_estimators.html",
    "title": "Estimators",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]\nTo recap, we have\nThe parameter \\(\\theta\\) is a function that could be computed if we knew the distribution of the variables (e.g., outcome regressions). Because we do not know this distribution, we need to estimate it from a sample of observed data. We now discuss how to estimate the parameter \\(\\theta\\) from a sample.",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#sequential-regression-estimator",
    "href": "info_estimators.html#sequential-regression-estimator",
    "title": "Estimators",
    "section": "Sequential Regression Estimator",
    "text": "Sequential Regression Estimator\nOne possible estimator is simply a plug-in estimator of the identification result. This estimator is often referred to as G-computation, or iterative conditional expectation (ICE), and it proceeds by simply estimating the regressions described in the identification result. Another way to describe this algorithm is as follows:\n\n\n\nInitialize \\(\\hat \\m_{\\tau +1} = Y_i\\).\nFor \\(t = \\tau, ..., 1\\):\n\nUsing a pre-specified parametric model, regress \\(\\hat \\m_{i,t+1}\\) on \\(\\{A_{i, t}, H_{i,t}\\}\\). This gives you a predictive model \\(\\hat \\m_t(a_t, h_t)\\).\nGenerate predictions from this model with \\(A_{i,t}\\) changed to \\(A^{\\dd}_{i,t}\\). Let \\(\\hat \\m_t(A_{i, t}^\\dd, H_{i,t})\\) be these predicted values.\nRepeat (iterate) the above two steps until generating predicted values \\(\\hat \\m_1(A_{i, 1}^\\dd, H_{i,1})\\) at the first time point.\n\nTake the final estimate as \\(\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n \\hat \\m_1(A_{i, 1}^\\dd, H_{i,1})\\).\nCompute standard errors using a bootstrap of steps 1 and 2; or a Delta method, if available.\n\n\n\nA substitution estimator is nice, because its estimates are guaranteed to stay within the valid range of the outcome and it is simple to estimate. That said, its cons are major:\n\nIn studies with multiple time points, the adjustment set can become large very quickly. For instance, consider a study with 3 covariates measured at every time point, and 10 time points. Even though we have only 3 covariates at each time point, the number of covariates in the regression of \\(Y\\) is 33.\nImagine trying to correctly specify (e.g., include the appropriate interactions) a parametric model (e.g., logistic, Cox) with 33 variables.\n\n\n\n\n\n\n\n\nPros ✅\nCons ❌\n\n\n\n\nSimple to implement\nConsistency requires correct estimation of all regressions\n\n\nSubstitution estimator\nCorrectness of bootstrap requires pre-specified parametric models",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#sec-density-ratio-estimation",
    "href": "info_estimators.html#sec-density-ratio-estimation",
    "title": "Estimators",
    "section": "Density-ratio estimation",
    "text": "Density-ratio estimation\n\n\n\n\n\n\nThe next three estimators all rely on estimating the density ratio\n\\[\nr_t(a_t, h_t) = \\frac{\\g_t^\\dd(a_t \\mid h_t)}{\\g_t(a_t \\mid h_t)}.\n\\]\nRecall that \\(\\g_t^\\dd(a_t \\mid h_t)\\) denotes the density of treatment post-intervention, i.e., the density of \\(\\dd(A_t, H_t)\\) evaluated at \\(a_t\\), and that \\(\\g_t(a_t \\mid h_t)\\) denotes the density of the observed treatment under no intervention.\nWe will often refer to this ratio as the intervention mechanism throughout the workshop. Estimation of \\(r_t(a_t, h_t)\\) is fully automated in lmtp and hidden from the user. A comprehensive understanding of this process isn’t necessary to use lmtp!\n\n\n\nWe can directly estimate this density ratio with a classification trick. To do this, we create an augmented dataset with \\(2n\\) observations. In this new dataset, the outcome is a new variable that we make, \\(\\Lambda\\), (defined below) and the predictors are the variables \\(A_t\\) and \\(H_t\\). The data structure at time \\(t\\) is then redefined as\n\\[\n(H_{\\lambda, i, t}, A_{\\lambda, i, t}, \\Lambda_{\\lambda, i} : \\lambda = 0, 1; i = 1, ..., n)\n\\]\n\n\\(\\Lambda_{\\lambda, i} = \\lambda_i\\) indexes duplicate values. So if \\(\\Lambda_i =1\\) if observation \\(i\\) is a duplicated value and \\(\\Lambda_i =0\\) otherwise.\nFor all duplicated observations \\(\\lambda\\in\\{0,1\\}\\) with the same \\(i\\), \\(H_{\\lambda, i, t}\\) is the same\nFor all the original observations, \\(\\lambda = 0\\), \\(A_{\\lambda=0, i, t}\\) equals the observed exposure values \\(A_{i, t}\\)\nFor all the duplicated observations, \\(\\lambda=1\\), \\(A_{\\lambda=1, i, t}\\) equals the exposure values under the intervention \\(\\dd\\), \\(A^{\\dd}_{i,t}\\)\n\n\n\n\nExample of augmenting data for density ratio estimation.\n\n\nWe then estimate the conditional probability that \\(\\Lambda=1\\) conditional on \\((A, H)\\) in this dataset, and divide it by the corresponding estimate of the conditional probability that \\(\\Lambda=0\\). Specifically, denoting \\(P^\\lambda\\) to be the distribution of the data in the augmented dataset, we have:\n\\[\n\\begin{align*}\nr_t(a_t, h_t) &= \\frac{\\g_t^\\dd(a_t \\mid h_t)}{\\g_t(a_t \\mid h_t)}\\\\\n&= \\frac{P^\\lambda(a_t, h_t \\mid \\Lambda = 1)}{P^\\lambda(a_t, h_t \\mid \\Lambda = 0)}\\\\\n    &= \\frac{P^\\lambda(\\Lambda = 1\\mid a_t,h_t)P^\\lambda(a_t,h_t)}{P^\\lambda(\\Lambda = 1)}\\times \\frac{P^\\lambda(\\Lambda = 0)}{P^\\lambda(\\Lambda = 0\\mid a_t,h_t)P^\\lambda(a_t,h_t)} \\\\\n    &=\\frac{P^\\lambda(\\Lambda = 1\\mid a_t,h_t)}{P^\\lambda(\\Lambda = 0\\mid a_t, h_t)}\n\\end{align*}\n\\]\n\nExample\nConsider the following example where we simulate 500 observations from this data-generating mechanism:\n\\[\\begin{align}\nL_1 &\\sim \\text{Normal}(0, 1) \\\\  \nA_1 &\\sim \\text{Bernoulli}(\\text{logit}^{-1}(-1 + 1.5L_1))\\\\  \nL_2 &\\sim \\text{Normal}(0, 1) \\\\\nA_2 &\\sim \\text{Bernoulli}(\\text{logit}^{-1}(-1 + 1.5L_2 + 2A_1))\\\\\nY &\\sim \\text{Normal}(-1 - 1.2L_1 + 2.4A_1 - 2L_2 + 1.2A_2)\n\\end{align}\\] The data has been loaded into R in the background as foo. We will use the density ratio classification trick to estimate the density ratio \\(r_t(a_t, h_t)\\) under the intervention \\[\n\\dd(a_t, \\epsilon_t) = \\begin{cases}\n0 &\\text{ if } \\epsilon_t &lt; 0.5 \\text{ and } a_t = 1 \\\\\na_t &\\text{ otherwise.}\n\\end{cases}\n\\] The true value under this intervention is approximately \\(-0.37\\). First, we create the augmented data. We then regress the \\(\\Lambda\\) on \\(A_t\\) and \\(H_t\\) and estimate the density ratio as the predicted odds.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#inverse-probability-weighting",
    "href": "info_estimators.html#inverse-probability-weighting",
    "title": "Estimators",
    "section": "Inverse Probability Weighting",
    "text": "Inverse Probability Weighting\nWe call this estimator IPW due to its similarity with the inverse-probability weighted estimator for binary treatments, but it would be more accurately referred to as simply reweighted estimator. It is based on the following alternative identification formula:\n\\[\n\\theta = \\E \\bigg[ \\bigg\\{\\prod_{t=1}^\\tau r_t(a_t, h_t) \\bigg\\} Y \\bigg]\n\\]\n\nThe algorithm is as follows:\n\n\n\nConstruct estimates of \\(r_{i,t}(a_t, h_t)\\) using the density ratio classification trick and a pre-specified parametric model.\nDefine the weights \\(w_{i} = \\prod_{t=1}^\\tau r_{i,t}(a_t, h_t)\\).\nTake the final estimate as \\(\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n \\hat{w}_{i}\\times y_i\\).\nCompute standard errors using a bootstrap of steps 1-3.\n\n\n\n\nExample\nLet’s apply the IPW estimator using the estimated density ratios we calculated in the previous example. We first compute the weights as the product of the density ratios.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nOur estimate is then the sample average of the estimated weights times the observed outcomes.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilarly to the sequential regression estimator, the IPW is simple to implement. However, it suffers from the same drawbacks.\n\n\n\n\n\n\n\n\nPros ✅\nCons ❌\n\n\n\n\nSimple to implement\nConsistency requires correct estimation of all regressions\n\n\n\nUncertainty quantification requires pre-specified parametric models",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#doubly-robust-estimators",
    "href": "info_estimators.html#doubly-robust-estimators",
    "title": "Estimators",
    "section": "Doubly Robust Estimators",
    "text": "Doubly Robust Estimators\nUncertainty quantification with G-computation and IPW estimators require the estimation of nuisance parameters with correctly specified parametric models. We will now turn our attention to two non-parametric estimators that allow us to use flexible regression tools:\n\ntargeted minimum-loss based estimator (TMLE), and\na sequentially doubly-robust estimator (SDR).\n\nWait, what does it mean for an estimator to be doubly robust?\n\nFor the simple case of a single time point, an estimator is considered doubly robust if it is able to produce a consistent estimate of the target parameter as long as either the outcome model is consistently estimated or the treatment (and censoring) model(s) are consistently estimated. For example:\n\n\n\n\n\nTime\n1\n\n\nTreatment\nCorrect\n\n\nOutcome\nWrong\n\n\n\n\n\n\n\nFor time-varying setting, an estimator is \\(\\tau + 1\\) doubly robust if, for some time \\(s\\), all outcome regressions for \\(t &gt;s\\) are consistently estimated and all intervention mechanisms (treatment + censoring) for \\(t \\leq s\\) are consistently estimated. Consider for example \\(5\\) time points:\n\n\n\n\nTime\n1\n2\n3\n4\n5\n\n\nTreatment\nCorrect\nCorrect\nWrong\nWrong\nWrong\n\n\nOutcome\nWrong\nWrong\nCorrect\nCorrect\nCorrect\n\n\n\n\n\n\nSequential double robustness (often also referred to as \\(2^\\tau\\)-multiply robust) implies that an estimator is consistent if for all times either the outcome or intervention mechanism (treatment + censoring) is consistently estimated. For example:\n\n\n\n\nTime\n1\n2\n3\n4\n5\n\n\nTreatment\nCorrect\nWrong\nCorrect\nWrong\nCorrect\n\n\nOutcome\nWrong\nCorrect\nWrong\nCorrect\nWrong\n\n\n\n\n\n\n\n\n\nBe careful not to confuse doubly robust consistency with rate double robustness. Doubly robust consistency refers to the properties we just described. Rate double robustness refers to the property where the error of an estimator is the product of errors in estimation of two nuisance functions.\nRate double robustness is perhaps more interesting practically as it implies that the error of the estimator can be small in cases where both models are wrong!\n\n\n\n\nEfficient Influence Function\nKey to constructing the TMLE and SDR is the efficient influence function (EIF).\n\n\n\n\n\n\n\nThe EIF characterizes the asymptotic behavior of all regular and efficient estimators.\nThe EIF characterizes the first-order bias of plug-in estimators of pathwise differentiable estimands.\n\n\n\n\nBefore we introduce the EIF, it’s necessary to make some additional assumptions on \\(A\\) and \\(\\dd\\).\n\nThe treatment \\(A\\) is discrete, or\nIf \\(A\\) is continuous, the function \\(\\dd\\) is piecewise smooth invertible\nThe function \\(\\dd\\) does not depend on the observed distribution \\(\\P\\) (this means that we cannot apply these methods to odds ratio IPSI, although we can apply them to risk ratio IPSI)\n\nThese assumptions ensure that the efficient influence function of \\(\\theta\\) for interventions \\(\\dd\\) have a structure similar to the influence function for the effect of dynamic regimes. This allows for multiply robust estimation, which is not generally possible for interventions \\(\\dd\\) that depend on \\(\\P\\).\nFor an observation \\(o\\) define the function\n\\[\n\\phi_t: o \\mapsto \\sum_{s=t}^\\tau \\bigg( \\prod_{k=t}^s r_k(a_k, h_k)\\bigg) \\big\\{\\m_{s+1}(a_{s+1}^\\dd, h_{s+1}) - \\m_s(a_s, h_s) \\big\\} + \\m_t(a_t^\\dd, h_t).\n\\]\nThe efficient influence function for estimating \\(\\theta = \\E[\\m_1(A^\\dd, L_1)]\\) in the non-parametric model is given by \\(\\phi_1(O) - \\theta\\).\nIn the case of single time-point, the influence function simplifies to\n\\[\nr(a, w)\\{Y - \\m(a,w)\\} + \\m(a^{\\dd},w) - \\theta.\n\\]",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#targeted-minimum-loss-based-estimation",
    "href": "info_estimators.html#targeted-minimum-loss-based-estimation",
    "title": "Estimators",
    "section": "Targeted Minimum-Loss Based Estimation",
    "text": "Targeted Minimum-Loss Based Estimation\nTMLE exploits the fact that the expected value of the EIF is equal to zero. Consider the EIF for the single time-point setting we just defined.\n\\[\\begin{aligned}\n\\E[\\phi_1(O) - \\theta] &= \\E[r(a, w)\\{Y - \\m(a,w)\\} + \\m(a^{\\dd},w) - \\theta]\\\\\n&= \\E[r(a, w)\\{Y - \\m(a,w)\\}] + \\theta - \\theta \\\\\n&= \\E[r(a, w)\\{Y - \\m(a,w)\\}] \\\\\n\\E[r(a, w)\\{Y - \\m(a,w)\\}] &= 0\n\\end{aligned}\\]\nThe key insight is to recognize that \\(r(a, w)\\{Y - \\m(a,w)\\}\\) looks like a score equation \\(\\sum_i r_i(a, w)\\{Y_i - \\m_i(a,w)\\} = 0\\) that can be solved using a GLM with weights \\(r_i(a, w)\\), offset \\(m_i(a,w)\\), an intercept, and the canonical link. Thus, by solving the score equation with a GLM, TMLE solves the EIF estimating equation.\nThe algorithm is as follows:\n\n\n\nConstruct estimates of \\(r_{i,t}(a_t, h_t)\\) using the density ratio classification trick and your favorite regression method.\nFor \\(t = 1, ..., \\tau\\), compute the weights: \\(w_{i,t} = \\prod_{k=1}^t r_{i,k}(a_{i,k}, h_{i,k})\\)\nSet \\(\\tilde{\\m}_{i,\\tau +1}(A^\\dd_{i,t+1}, H_{i,t+1}) = Y_i\\).\nFor \\(t = \\tau, ..., 1\\):\n\nRegress \\(\\tilde{\\m}_{i,t+1}(A^\\dd_{i,t+1}, H_{i,t+1})\\) on \\(\\{A_{i, t}, H_{i,t}\\}\\).\n\nUsing this regression, generate predictions from \\(\\{A_{i, t}, H_{i,t}\\}\\) and \\(\\{A^\\dd_{i, t}, H_{i,t}\\}\\).\nDenote the predictions as \\(\\tilde{\\m}_t(A_{i,t}, H_{i,t})\\) and \\(\\tilde{\\m}_t(A^\\dd_{i,t}, H_{i,t})\\) respectively.\n\nFit the generalized linear tilting model:\n\\(\\text{link }\\tilde{\\m}^\\epsilon_t(A_{i,t}, H_{i,t}) = \\epsilon + \\text{link }\\tilde{\\m}_{i,t}(A_{i,t}, H_{i,t})\\)\nwith weights \\(w_{i,t}\\).\n\n\\(\\text{link }\\tilde{\\m}_{i,t}(A_{i,t}, H_{i,t})\\) is an offset variable (i.e., a variable with known parameter value equal to one).\nThe parameter \\(\\epsilon\\) may be estimated by running a generalized linear model of \\(\\tilde{\\m}_{i,t+1}(A^\\dd_{i,t+1}, H_{i,t+1})\\) with only an intercept term, an offset term equal to \\(\\text{link }\\tilde{\\m}_{i,t}(A_{i,t}, H_{i,t})\\), and weights \\(w_{i,t}\\).\n\nLet \\(\\hat\\epsilon\\) be the maximum likelihood estimate, and update the estimates as:\n\\(\\text{link }\\tilde{\\m}^\\hat\\epsilon_t(A^\\dd_{i,t}, H_{i,t}) = \\hat\\epsilon + \\text{link }\\tilde{\\m}_t(A^\\dd_{i,t}, H_{i,t})\\)\n\\(\\text{link }\\tilde{\\m}^\\hat\\epsilon_t(A_{i,t}, H_{i,t}) = \\hat\\epsilon + \\text{link }\\tilde{\\m}_t(A_{i,t}, H_{i,t})\\)\nUpdate \\(\\tilde{\\m}_{i,t} = \\tilde{\\m}^\\hat\\epsilon_{i,t}\\), \\(t = t-1\\), and iterate.\n\nThe final estimate is defined as \\(\\hat\\theta = \\frac{1}{n}\\sum_{i=1}^n\\tilde{m}_{i, 1}(A^\\dd_{i, 1}, L_{i, 1})\\).\n\n\n\n\nExample\nLet’s apply TMLE using the estimated density ratios we calculated in the previous example.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThe benefits of TMLE are quite large as it solves the main problems with g-computation and IPW. Namely, the TMLE is \\(\\tau+1\\) doubly robust and rate doubly robust.\n\n\n\n\n\n\n\nPros ✅\nCons ❌\n\n\n\n\nSubstitution estimator\nnot sequentially doubly-robust\n\n\n\\(\\tau+1\\) doubly-robust\n\n\n\ncan use machine learning",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#sequentially-doubly-robust-estimator",
    "href": "info_estimators.html#sequentially-doubly-robust-estimator",
    "title": "Estimators",
    "section": "Sequentially Doubly Robust Estimator",
    "text": "Sequentially Doubly Robust Estimator\nThe SDR estimator is based on the central fact that\n\\[\nE[\\phi_t(O)\\mid A_t=a_t, H_t=h_t] \\approx \\m_t(a_t, h_t)\n\\]\nwhere the approximation error is “doubly robust”. This provides a way to obtain doubly robust estimators of the regression functions \\(\\m_t(a_t, h_t)\\)\nThe estimation algorithm is as follows:\n\n\n\nConstruct estimates of \\(r_{i,t}(a_t, h_t)\\) using the density ratio classification trick and your favorite regression method.\nInitialize \\(\\phi_{\\tau +1}(O_i) = Y_i\\).\nFor \\(t = \\tau, ..., 1\\):\n\nCompute the pseudo-outcome \\(\\check{Y}_{i,t+1} = \\phi_{t+1}(O_i)\\).\nRegress \\(\\check{Y}_{i,t+1}\\) on \\(\\{A_{i, t}, H_{i,t}\\}\\). Let \\(\\check\\m_{t}\\) denote this regression.\nIterate.\n\nThe final estimate is defined as \\(\\hat\\theta = \\frac{1}{n}\\sum_{i=1}^n\\phi_1(O_i)\\), where \\(\\phi_1\\) is computed using \\(\\hat r_t\\) and \\(\\check\\m_{t}\\).\n\n\n\n\nExample\nLet’s apply the SDR estimator. Again, we’ll use the estimated density ratios we previously estimated.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nSimilarly to TMLE, the SDR estimator addresses the main issues with g-computation and IPW. In addition, the SDR estimator is sequentially doubly robust. However, this comes at the price of the SDR not being a substitution estimator, meaning it could produce a point estimate outside the valid range of the outcome.\n\n\n\n\n\n\n\nPros ✅\nCons ❌\n\n\n\n\n\\(\\tau+1\\) doubly-robust\nnot a substitution estimator\n\n\nsequentially doubly-robust\n\n\n\ncan use machine learning",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#choosing-an-estimator",
    "href": "info_estimators.html#choosing-an-estimator",
    "title": "Estimators",
    "section": "Choosing an Estimator",
    "text": "Choosing an Estimator\n\nIn general we never recommend using the IPW or sequential regression estimator. Both require the use of correctly pre-specified parametric models for valid statistical inference 🙃.\nThe TMLE and SDR estimators, however, are both doubly or sequentially doubly robust and can be used with machine-learning algorithms while remaining \\(\\sqrt{n}\\)-consistent under reasonable assumptions.\n\n\nTable 1. Summary of estimator properties.\n\n\n\n\n\n\n\n\n\n\nIPW\nG-comp.\nTMLE\nSDR\n\n\n\n\nUses outcome regression\n\n⭐\n⭐\n⭐\n\n\nUses the propensity score\n⭐\n\n⭐\n⭐\n\n\nValid inference with machine-learning\n\n\n⭐\n⭐\n\n\nSubstitution estimator\n\n⭐\n⭐\n\n\n\n\\(\\tau+1\\) doubly robust\n\n\n⭐\n⭐\n\n\nSequentially doubly robust\n\n\n\n⭐\n\n\n\n\n\n\n\n\n\nQuestion: Why can’t we use machine learning with the G-computation and IPW estimators?\n\n\n✅ Answer\n\nThe G-computation and IPW estimators require models that converge to the truth at a \\(\\sqrt{n}\\)-rate. Machine learning algorithms are not guaranteed to do this.\n\n\n\n\nWhile the SDR estimator may be more robust to model misspecification, the TMLE does have the advantage of being a substitution estimator. Because of this, estimates from the TMLE are guaranteed to stay within the valid range of the outcome. Taken together, this leads to the following recommendations for choosing between the TMLE and SDR:\n\n\n\n\n\n\n\nIf treatment is not time-varying, use the TMLE.\nIf treatment is time-varying and the parameter \\(\\theta\\) has clear bounds, such as probabilities, beware of the SDR estimator. Use TMLE preferably.",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "info_estimators.html#cross-fitting",
    "href": "info_estimators.html#cross-fitting",
    "title": "Estimators",
    "section": "Cross-fitting",
    "text": "Cross-fitting\nWhen estimating nuisance parameters with data adaptive algorithms, you should perform a process similar to cross-validation called cross-fitting. Cross-fitting helps ensure:\n\nthat standard errors will be correct, and\ncan help reduce estimator bias and improve coverage of the confidence intervals.\n\nCross-fitting is fully automated in lmtp, but for more information we recommend reviewing Chernozhukov et al. (2018), Dı́az (2020), and Zivich and Breskin (2021).",
    "crumbs": [
      "Instructors",
      "3. Estimators"
    ]
  },
  {
    "objectID": "lmtp.html",
    "href": "lmtp.html",
    "title": "The lmtp package",
    "section": "",
    "text": "So far, we’ve\n\nDefined a causal estimand for general, hypothetical interventions\nIdentified that estimand from observational data\nDiscussed 4 different estimators.\n\nIt’s now time to move on to applying this new knowledge to real data examples.\n\nExisting software and lmtp\nGeneral software for estimation of causal effects in longitudinal studies is limited:\n\nThe ipw and gfoRmula packages provide routines for estimating causal effects using inverse probability weighting (IPW) and the parametric g-formula respectively. As we already discussed, however, the validity of these methods requires making unrealistic parametric assumptions.\nThe ltmle and survtmle packages implement a doubly-robust method for estimating causal effects from longitudinal data, but these packages do not support continuous valued exposures, multiple exposures, interventions that depend on the natural value of the exposure, or stochastic interventions.\n\nIn contrast, the lmtp package (software companion to the paper by Dı́az et al. (2023)) can be used to doubly robustly and flexibly estimate point-in-time and longitudinal effects of the general set of interventions that we’ve talked about today.\n\nlmtp could be your default package for conducting causal analyses in R!\n\n\n\nInstallation\nFor this workshop, lmtp has already been installed with webr. However, you can install the package locally on your machine from CRAN with\n\n\ninstall.packages(\"lmtp\")\n\nAlternatively, you can download the developmental version from GitHub with\n\n# install.packages(devtools)\ndevtools::install_github(\"nt-williams/lmtp@devel\")\n\n\n\nPreliminaries\nBefore we move on to using lmtp, here’s general information that will be applicable across all examples:\n\nData is passed to estimators through the data argument.\nData should be in wide format with one column per variable per time point under study (i.e., there should be one column for every variable in \\(O\\)). Data may be either a data.frame or tibble but not a data.table.\nColumns do not have to be in any specific order and the data may contain variables that are not used in estimation.\nThe names of treatment variables are specified with the trt argument.\nThe names of censoring variables are specified with the cens argument.\nThe names of baseline covariates are specified with the baseline argument.\nThe names of time-varying covariates are specified with the time_vary argument.\nThe trt argument accepts either a character vector or a list of character vectors.\nThe cens and baseline arguments accept character vectors.\nThe time_vary argument accepts a list of character vectors.\n\n\n\n\n\n\n\nVectors and lists are basic data structures in R. A vector can be thought of as a 1-dimensional collection of homogeneous elements. For example a character vector is a collection of values with the class character.\n\n\na_character_vector &lt;- c(\"this\", \"is\", \"a\", \"character\", \"vector\")\n\nA list is similar, except that it can hold elements of different types.\n\n\na_list &lt;- list(\n  a_character_vector = c(\"this\", \"is\", \"a\", \"character\", \"vector\"), \n  a_numeric_vector = 1:10, \n  a_list_whithin_a_list = list(1, 2, 3, 4)\n)\n\n\n\n\n\nThe trt, cens, and time_vary arguments must be sorted according to the time-ordering of the model with each index containing the name (or names) of variables for the given time.\nThe outcome variable is specified with the outcome argument.\nThe outcome_type argument specifies the type of outcome. It should be set to \"continuous\" for continuous outcomes, \"binomial\" for dichotomous outcomes, and \"survival\" for time-to–event outcomes.\nCensoring indicators should be coded using 0 and 1 where 1 indicates an observation is observed at the next time and 0 indicates loss-to-follow-up. Once an observation’s censoring status is switched to 0 it cannot change back to 1. Missing data before an observation is censored is not allowed.\nThe argument should be set to for continuous outcomes, for dichotomous outcomes, and for time-to-event outcomes.\n\n\n\nData structure examples\n\nPoint treatment\n\n\n\nAdapted from Hoffman et al., 2022.\n\n\n\n\nPoint-treatment with censoring\n\n\n\nAdapted from Hoffman et al., 2022.\n\n\n\n\nTime-varying treatment with censoring\n\n\n\nAdapted from Hoffman et al., 2022.\n\n\n\n\nPoint-treatment with survival outcome\n\n\n\nAdapted from Hoffman et al., 2022.\n\n\n\n\nTime-varying treatment with survival outcome\n\n\n\nAdapted from Hoffman et al., 2022.\n\n\n\n\n\nMachine learning ensembles\nAs was already discussed, an attractive property of multiply-robust estimators is that they can incorporate flexible machine-learning algorithms for the estimation of nuisance parameters while remaining \\(\\sqrt{n}\\)-consistent.\n\nlmtp uses the super learner algorithm for estimating these nuisance parameters.\nThe super learner algorithm is an ensemble learner than incorporates a set of candidate models through a weighted convex-combination based on cross-validation. Asymptotically, this weighted combination of models, called the meta-learner, will outperform any single one of its components.\nlmtp uses the implementation of the super learner provided by the SuperLearner package.\nThe algorithms to be used in the super learner are specified with the learners_trt and learners_outcome arguments.\nThe outcome variable type should guide users on selecting the appropriate candidate learners for use with the learners_outcome argument.\nRegardless of whether an exposure is continuous, dichotomous, or categorical, the exposure mechanism is estimated using classification. Therefore only include candidate learners capable of binary classification with the learners_trt argument.\nCandidate learners that rely on cross-validation for the tuning of hyper-parameters should support grouped data if used with learners_trt. Because estimation of the treatment mechanism relies on the augmented \\(2n\\) duplicated data set, duplicated observations must be put into the same fold during sample-splitting. This is done automatically by the package.\n\n\n\n\n\n\nReferences\n\nDı́az, Iván, Nicholas Williams, Katherine L Hoffman, and Edward J Schenck. 2023. “Nonparametric Causal Effects Based on Longitudinal Modified Treatment Policies.” Journal of the American Statistical Association 118 (542): 846–57.\n\n\nVan der Laan, Mark J, Eric C Polley, and Alan E Hubbard. 2007. “Super Learner.” Statistical Applications in Genetics and Molecular Biology 6 (1).\n\n\nWilliams, Nicholas, and Iván Dı́az. 2023. “Lmtp: An R Package for Estimating the Causal Effects of Modified Treatment Policies.” Observational Studies 9 (2): 103–22.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "The lmtp package"
    ]
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Nick Williams\nNick is a Senior Data Analyst in Columbia University’s Mailman School of Publich Health, Department of Epidemiology. His interests are in the development of statistical computing tools for novel causal inference methods. He’s the author and maintainer of multiple R packages.\n\n\n\nKara Rudolph\nKara is an Assistant Professor of Epidemiology at Columbia University, Mailman School of Public Health. Her research interests are in developing and applying causal inference methods to understand social and contextual influences on mental health, substance use, and violence. Her current work focuses on developing and applying methods for transportability and mediations to understand mechanisms relevant for drug use disorder prevention and treatment in various target populations.\n\n\n\nIván Díaz\nIván is an Associate Professor of Biostatistics at New York University Grossman School of Medicine. His research focuses on the development of non-parametric statistical methods for causal inference from observational and randomized studies with complex datasets, using machine learning. This includes but is not limited to mediation analysis, methods for continuous exposures, longitudinal data including survival analysis, and efficiency guarantees with covariate adjustment in randomized trials. He also works applying these methods to healthcare research, including in neurology, critical care, opioid use research, and other areas."
  },
  {
    "objectID": "R_multivariate.html",
    "href": "R_multivariate.html",
    "title": "Multivariate exposures",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Multivariate Exposures"
    ]
  },
  {
    "objectID": "R_multivariate.html#niehs-simulation-data",
    "href": "R_multivariate.html#niehs-simulation-data",
    "title": "Multivariate exposures",
    "section": "NIEHS Simulation Data",
    "text": "NIEHS Simulation Data\nFor our example of estimating the effects of simultaneous interventions on multiple variables, we will use simulated data from the 2015 NIEHS Mixtures Workshop. The data has already been loaded into R in the background as mixtures. You can view and download the raw data here.\n\nThe simulated data has \\(n = 500\\) observations and is intended to replicate a prospective cohort study.\nThe data is composed of 7 log-normally distributed and correlated exposures variables (\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\"), a single continuous outcome (\"Y\"), and one binary confounder (\"Z\").\nThere is no missing covariate data, no measurement error, and no censoring.\n\n\n\n\n\n\n\n\n\n\n\n\nOnly exposure variables X1, X2, X4, X5, and X7 have an effect on the outcome Y. However, the direction of the effects varies.\nX1, X2, and X7 are positively associated with the outcome.\nX4 and X5 are negatively associated with the outcome.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Multivariate Exposures"
    ]
  },
  {
    "objectID": "R_multivariate.html#multivariate-shift-functions",
    "href": "R_multivariate.html#multivariate-shift-functions",
    "title": "Multivariate exposures",
    "section": "Multivariate shift functions",
    "text": "Multivariate shift functions\n\n\n\n\n\n\nOnly two things need to change when using lmtp estimators with multivariate treatments:\n\nInstead of a vector, you should now pass a list to the trt argument\nThe shift function should return a named list of vectors instead of a single vector.\n\n\n\n\nLet’s use lmtp to estimate the effect of a modified treatment policy which intervenes on all 7 exposure simultaneously on the outcome:\n\\[\n\\dd(\\mathbf{a}, h) =\n\\begin{cases} \\dd(a_1, h) =\n\\begin{cases}\na_1 - 0.2 &\\text{ if } a_1 - 0.2 &gt; 0 \\\\\na_1 &\\text{ otherwise }\n\\end{cases} \\\\\n\\dd(a_2, h) =\n\\begin{cases}\na_2 - 0.4 &\\text{ if } a_2 - 0.4 &gt; 0 \\\\\na_2 &\\text{ otherwise }\n\\end{cases} \\\\\n\\dd(a_3, h) = a_3 + 0.4 \\\\\n\\dd(a_4, h) = a_4 + 0.1 \\\\\n\\dd(a_5, h) = a_5 + 0.5 \\\\\n\\dd(a_6, h) =\n\\begin{cases}\na_6 - 0.2 &\\text{ if } a_6 - 0.2 &gt; 0 \\\\\na_6 &\\text{ otherwise }\n\\end{cases} \\\\\n\\dd(a_7, h) =\n\\begin{cases}\na_7 - 0.3 &\\text{ if } a_7 - 0.3 &gt; 0 \\\\\na_7 &\\text{ otherwise }\n\\end{cases}\n\\end{cases}\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Multivariate Exposures"
    ]
  },
  {
    "objectID": "R_multivariate.html#problem-1",
    "href": "R_multivariate.html#problem-1",
    "title": "Multivariate exposures",
    "section": "Problem 1",
    "text": "Problem 1\nUsing TMLE, estimate the population mean outcome under the simultaneous intervention we just defined. Fit both the treatment mechanism and the outcome regression using this set of learners: c(\"SL.mean\", \"SL.glm\", \"SL.gam\", \"SL.rpart\", \"SL.rpartPrune\", \"SL.step.interaction\"). Assign the result to ans. To save time, don’t use crossfitting; lmtp has already been loaded into the R session.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nset.seed(4363754)\n\nlearners &lt;- c(\"SL.mean\", \n              \"SL.glm\", \n              \"SL.gam\", \n              \"SL.rpart\", \n              \"SL.rpartPrune\", \n              \"SL.step.interaction\")\n\nans &lt;- lmtp_tmle(data = mixtures, \n                 trt = A, \n                 outcome = \"Y\", \n                 baseline = \"Z\", \n                 shift = d, \n                 mtp = TRUE,\n                 outcome_type = \"continuous\",\n                 learners_trt = learners, \n                 learners_outcome = learners, \n                 folds = 1)\n\nprint(ans)",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Multivariate Exposures"
    ]
  },
  {
    "objectID": "R_multivariate.html#problem-2",
    "href": "R_multivariate.html#problem-2",
    "title": "Multivariate exposures",
    "section": "Problem 2",
    "text": "Problem 2\nCompared to what was observed under the natural course of exposure, how did intervening upon the set of exposures effect the outcome? Estimate this effect using lmtp_contrast().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Answer\n\n\nobs_y &lt;- mean(mixtures$Y)\nlmtp_contrast(ans, ref = obs_y)",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Multivariate Exposures"
    ]
  },
  {
    "objectID": "macros.html",
    "href": "macros.html",
    "title": "Beyond the \n Average Treatment Effect",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]"
  },
  {
    "objectID": "R_static.html",
    "href": "R_static.html",
    "title": "Static Interventions",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#synthetic-covid-rct-data",
    "href": "R_static.html#synthetic-covid-rct-data",
    "title": "Static Interventions",
    "section": "Synthetic COVID RCT Data",
    "text": "Synthetic COVID RCT Data\nFor our first example, we’ll use a synthetic dataset of \\(n = 1000\\) patients from a hypothetical clinical trial for a treatment to decrease intubation and death among patients hospitalized with COVID-19.\n\nThe data is based on a database of over 1,500 patients hospitalized at Weill Cornell Medicine New York Presbyterian Hospital prior to 15 May 2020 with COVID-19 confirmed through PCR.\nTo replicate a two-arm randomized clinical trial (RCT), we’ve simulated a hypothetical treatment variable (A) that is randomly assigned for each observation with probability 0.5.\nThe outcome of interest (event) is intubation or death by 15-days post-hospitalization.\nTreatment is associated with increased survival.\nBaseline covariates include: age, sex, BMI, smoking status, whether the patient required supplemental oxygen within 3 hours of presenting to the emergency department, number of comorbidities, number of relevant symptoms, presence of bilateral infiltrates on chest X-ray, dyspnea, and hypertension.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#population-mean-outcomes",
    "href": "R_static.html#population-mean-outcomes",
    "title": "Static Interventions",
    "section": "Population mean outcomes",
    "text": "Population mean outcomes\nOur goal is to estimate the average treatment effect (ATE) of the hypothetical treatment on intubation or death:\n\\[\n\\begin{align}\n\\theta &= \\E(Y^{A=1} - Y^{A=0}) \\\\\n&= \\textcolor{blue}{\\E(Y^{A=1})} - \\textcolor{red}{\\E(Y^{A=0})}\n\\end{align}\n\\]\nNotice that the ATE is composed of two parameters:\n\nthe proportion of patients who receive intubation or die in a hypothetical world where all patients receive treatment (\\(\\textcolor{blue}{\\E(Y^{A=1})}\\)), and\nthe proportion of patients who receive intubation or die in a hypothetical world where no patients receive treatment (\\(\\textcolor{red}{\\E(Y^{A=0})}\\)).\n\nWe refer to these parameters as population mean outcomes. Let’s rewrite the target parameter as a function of interventions using the framework we previously discussed. Let \\(\\dd_1(a, h) = 1\\) and \\(\\dd_0(a, h) = 0\\). Then,\n\\[\n\\theta = \\textcolor{blue}{\\E(Y^{\\dd_1})} - \\textcolor{red}{\\E(Y^{\\dd_0})}.\n\\]\nRemember, we refer to these interventions as static because they return the same value regardless of their input.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#writing-shift-functions",
    "href": "R_static.html#writing-shift-functions",
    "title": "Static Interventions",
    "section": "Writing shift functions",
    "text": "Writing shift functions\nSo, how do we translate the functions \\(\\dd_1\\) and \\(\\dd_0\\) into R code to be used with lmtp?\n\n\n\n\n\n\nPolicies can be specified using one of two arguments in lmtp estimators: shift or shifted.\n\n\n\nIf using shift, we supply a two-argument function of the form\n\nd &lt;- function(data, trt) {\n  # Insert code here\n}\n\n\nThe first argument should correspond to a dataset containing all the variables in \\(O\\).\nThe second argument should expect a string corresponding to the variable(s) \\(A_t\\) in \\(O\\).\nThis function should return a size \\(n\\) vector with the same class as \\(A_t\\) modified according to \\(d_t\\).\n\nFor \\(\\dd_1\\) this function would be:\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIf we apply this function to our data we are returned a vector of 1s with the same length as the number of rows in the observed data.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion: Which estimator should we use to estimate these parameters?\n\n✅ Answer\nBecause this is a non-time-varying study, we should use TMLE.\n\n\n\n\n\n\nLet’s now estimate \\(\\E(Y^{\\dd_1})\\) using lmtp.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nInstead of specifying an intervention using shift, we could instead supply a modified version of data to the shifted argument where the variables \\(A_t\\) are replaced with \\(A^d_t\\).\n\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n\n\n\n\nQuestion: How can we interpret this result?\n\n✅ Answer\nIn a hypothetical world where all patients received treatment, the expected proportion of patients who receive invasive mechanical ventilation by day 15 of hospitalization is 0.28 (95% CI: 0.24 to 0.32).",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#lmtp-objects",
    "href": "R_static.html#lmtp-objects",
    "title": "Static Interventions",
    "section": "lmtp objects",
    "text": "lmtp objects\nA call to an lmtp estimator returns a list of class lmtp.\n\nValues returned in an lmtp object.\n\n\n\n\n\n\nValue\nDescription\n\n\n\n\nestimator\nThe estimator used.\n\n\nestimate\nThe estimated population LMTP effect as an ife object.\n\n\nshift\nThe shift function specifying the treatment policy of interest.\n\n\noutcome_reg\nAn \\(n \\times \\tau + 1\\) matrix of outcome regression predictions. The mean of the first column is used for calculating theta.\n\n\ndensity_ratios\nAn \\(n \\times \\tau\\) matrix of the estimated, non-cumulative, density ratios.\n\n\nfits_m\nA list the same length as folds, containing the fits at each time-point for each fold for the outcome regression.\n\n\nfits_r\nA list the same length as folds, containing the fits at each time-point for each fold of density ratio estimation.\n\n\noutcome_type\nThe outcome variable type.\n\n\n\nLet’s inspect some of these values.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#problem-1",
    "href": "R_static.html#problem-1",
    "title": "Static Interventions",
    "section": "Problem 1",
    "text": "Problem 1\nWrite a function to estimate the population mean outcome if no patients received treatment. Assign the result to an object named dont_treat.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nd0 &lt;- function(data, trt) {\n  rep(0, nrow(data))\n}\n\ndont_treat &lt;- lmtp_tmle(\n  data = covid, \n  trt = \"A\", \n  outcome = \"event\", \n  baseline = W, \n  outcome_type = \"binomial\", \n  shift = d0, \n  folds = 1, \n  learners_trt = \"SL.glm\", \n  learners_outcome = \"SL.glm\"\n)\n\nprint(dont_treat)\n\n\n\n\n\n\n\n\nlmtp already implements d1 and d0 as static_binary_on() and static_binary_off()!",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_static.html#average-treatment-effect",
    "href": "R_static.html#average-treatment-effect",
    "title": "Static Interventions",
    "section": "Average treatment effect",
    "text": "Average treatment effect\nWith estimates of \\(\\E(Y^{\\dd_1})\\) and \\(\\E(Y^{\\dd_0})\\) we can now calculate the ATE. To do so, we’ll use the function lmtp_contrast().\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nIn addition to the ATE, we can also calculate the causal risk ratio and the causal odds ratio.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nConfirming what we already know, we’ve estimated the treatment as being associated with decreased intubation or death.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Static effects and the ATE"
    ]
  },
  {
    "objectID": "R_dtr.html",
    "href": "R_dtr.html",
    "title": "Dynamic treatment regimes",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]\nRecall that interventions where the output of the function depends only on covariates are referred to as dynamic interventions or dynamic treatment regimes. Let’s learn how to estimate the effects of dynamic treatment regimes using lmtp.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Dynamic Treatment Regimes"
    ]
  },
  {
    "objectID": "R_dtr.html#problem-1",
    "href": "R_dtr.html#problem-1",
    "title": "Dynamic treatment regimes",
    "section": "Problem 1",
    "text": "Problem 1\nWrite a shift function that assigns meal replacement to all observations at time 1, but only meal replacement at time 2 to those observations whose 4-month BMI is greater than 30.\n\\[\n\\dd_t(a_t, h_t, \\epsilon_t) = \\begin{cases}\n\\text{Meal replacement} &\\text{ if } t = 1 \\\\\n\\text{Meal replacement} &\\text{ if } t = 2 \\text{ and } \\text{4-month BMI} &gt; 30 \\\\\n\\text{Calorie deficit} &\\text{ otherwise.}\n\\end{cases}\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nd_dtr &lt;- function(data, trt) {\n  if (trt == \"A1\") return(rep(\"MR\", nrow(data)))\n  \n  ifelse(data$month4BMI &gt; 30, \"MR\", \"CD\")\n}\n\n\nLet’s estimate the effect of the dynamic treatment regime on the 12-month BMI using lmtp with the SDR estimator.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Dynamic Treatment Regimes"
    ]
  },
  {
    "objectID": "R_dtr.html#problem-2",
    "href": "R_dtr.html#problem-2",
    "title": "Dynamic treatment regimes",
    "section": "Problem 2",
    "text": "Problem 2\nSuppose we are interested in comparing the dynamic treatment regime to a static treatment regime where all patients receive meal replacement at both time points. Using the SDR estimator, estimate the effect of this static intervention.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\nfit_MR &lt;- lmtp_sdr(\n  data = bmi, \n  trt = c(\"A1\", \"A2\"), \n  outcome = \"month12BMI\", \n  baseline = c(\"gender\", \"race\", \"parentBMI\"), \n  time_vary = list(\"baselineBMI\", \"month4BMI\"),\n  shift = \\(data, trt) rep(\"MR\", nrow(data)), \n  outcome_type = \"continuous\",\n  folds = 1,\n  learners_trt = \"SL.glm\", \n  learners_outcome = c(\"SL.mean\", \"SL.glm\", \"SL.gam\")\n)\n\n\nLet’s also estimate the effect of an intervention where all patients receive a calorie deficit diet at both time points.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nFinally, let’s compare the dynamic treatment regime and the always receive meal replacement intervention to the always receive a calorie deficit intervention.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "Dynamic Treatment Regimes"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimating the causal effects of binary, categorical, continuous, and multivariate exposures in R",
    "section": "",
    "text": "In this workshop, we present methods to define and estimate the causal effects of categorical, continuous, and multivariate exposures. The methods are based on a generalization of the static and dynamic interventions that may be familiar to some of you. This generalization has been recently called modified treatment policies (MTPs). MTPs are hypothetical interventions where the post-intervention exposure is defined as a modification of the natural value of the exposure that can also depend on the unit’s history. This short course will introduce the lmtp R package for estimating the causal effects of these general estimand in both point-treatment and longitudinal studies. We will discuss identification of MTPs, estimation with a targeted minimum-loss based estimator and a sequentially doubly-robust estimator, and provide guidance on estimator choice and software usage.\n\n\n\nConference\nYear\n📍Location\n\n\n\n\nSER\n2024\nAustin, TX\n\n\nACIC\n2025\nDetroit, MI\n\n\nSER\n2025\nBoston, MA\n\n\nLiDS\n2025\nBrooklyn, NY\n\n\n\n\nLearning objectives\nBy the end of the workshop, participants will be able to:\n\nGeneralize static and dynamic interventions intuitively and using notation.\nEstimate the effect of a static or dynamic intervention with lmtp for point-treatment and longitudinal studies.\nEstimate the effect of an MTP on a continuous-valued exposure with lmtp for point-treatment and longitudinal studies.\nEstimate the effect of multivariate exposures with lmtp for point-treatment and longitudinal studies.\n\n\n\n\n\n\n\nThis workshop assumes the participant has a basic understanding of fundamental concepts in causal inference such as the concept of counterfactuals, and some experience with the R programming language.\n\n\n\n\n\nTentative Schedule\n\n\n\nTopic\nDuration\n\n\n\n\nIntroductions\n15 minutes\n\n\nFrom observed data to causal estimands\n15 minutes\n\n\nDefining causal effects using MTPs\n1 hour\n\n\nThe estimator landscape\n20 minutes\n\n\nBreak\n10 minutes\n\n\nSetting up the correct data structure\n15 minutes\n\n\nEstimating effects using the lmtp package\n1 hour 30 min\n\n\nQ + A\n15 minutes\n\n\n\n\n\nwebR\nThis workshop was prepared using Quarto and webR. The source code is available on GitHub. webR is a version of the R programming language compiled to be run directly in the browser. Using webR for this workshop avoids having to spend time setting up a computing environment and making sure workshop participants are using the same version of R and R packages.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.",
    "crumbs": [
      "Instructors",
      "Welcome!"
    ]
  },
  {
    "objectID": "info_conclusion.html",
    "href": "info_conclusion.html",
    "title": "Concluding Thoughts",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]",
    "crumbs": [
      "Instructors",
      "5. Final Remarks"
    ]
  },
  {
    "objectID": "info_conclusion.html#thank-you",
    "href": "info_conclusion.html#thank-you",
    "title": "Concluding Thoughts",
    "section": "Thank you!",
    "text": "Thank you!\nThank you for attending this workshop! We encourage you to reach out to us with any questions or feedback.",
    "crumbs": [
      "Instructors",
      "5. Final Remarks"
    ]
  },
  {
    "objectID": "info_conclusion.html#including-in-a-manuscript",
    "href": "info_conclusion.html#including-in-a-manuscript",
    "title": "Concluding Thoughts",
    "section": "Including in a manuscript",
    "text": "Including in a manuscript\nIf you decide to use lmtp in your research, here’s a summary that can be included for explaining the methodology. It is based on the explanation in Rudolph et al. (2024):\n\nWe estimated the effect of [exposure] on [outcome], adjusting for covariates. This effect can be written: \\(\\E(Y^{\\dd, C=1} - Y^{C=1}),\\) [change the preceding to correspond to your effect] where \\(\\E(Y^{C=1})\\) [change as applicable] denotes the expected value of the counterfactual outcome had the exposure not been intervened on (i.e., remained as observed) and had no one been censored, and where \\(\\E(Y^{\\dd, C=1})\\) [change as applicable] denotes the expected value of the counterfactual outcome had the exposure been intervened on as dictated by the function \\(\\dd(A)\\) and had no one been censored. We defined \\(\\dd(A)\\) as a hypothetical intervention that increased the value of the exposure by 20% (i.e., multiplied each person’s value by 1.2). [change the preceding to correspond to your intervention, \\(\\dd\\)]\nThe above statistical estimand is a type of ``modified treatment policy’’ (Haneuse and Rotnitzky (2013), Muñoz and Van Der Laan (2012), Young, Hernán, and Robins (2014), Dı́az et al. (2023)). The statistical estimand can be interpreted causally under the identifying assumptions of: 1) conditional exchangeability, meaning that there is no unobserved/unmeasured confounding of the relationship between the set of treatments and outcome conditional on covariates and that there is no unobserved/unmeasured confounding between censoring and the outcome conditional on the covariates and treatment; 2) positivity, and 3) consistency.\nWe estimated this statistical estimand using a doubly robust, nonparametric targeted minimum loss-based estimator (Dı́az et al. (2023), Williams and Dı́az (2023)). A cross-fitted version of this estimator was used with [fill in number]-folds. This estimator fits regressions for the outcome mechanism, treatment mechanism, and censoring mechanism. These regressions were fit using an ensemble of machine learning algorithms (van der Laan, Polley, and Hubbard (2007)) consisting of [fill in the algorithms included in your superlearner library].\n\nYou can cite the package using the following BibTeX entries:\n@article{diaz2023nonparametric,\n  title={Nonparametric causal effects based on longitudinal modified treatment policies},\n  author={D{\\'\\i}az, Iv{\\'a}n and Williams, Nicholas and Hoffman, Katherine L and Schenck, Edward J},\n  journal={Journal of the American Statistical Association},\n  volume={118},\n  number={542},\n  pages={846--857},\n  year={2023},\n  publisher={Taylor \\& Francis}\n}\n@article{williams2023lmtp,\n  title={lmtp: An {R} package for estimating the causal effects of modified treatment policies},\n  author={Williams, Nicholas and D{\\'\\i}az, Iv{\\'a}n},\n  journal={Observational Studies},\n  volume={9},\n  number={2},\n  pages={103--122},\n  year={2023},\n  publisher={University of Pennsylvania Press}\n}\nYou can reference this workshop with:\n@inproceedings{lmtpSER,\n  author = {Williams, Nicholas and D{\\'\\i}az, Iv{\\'a}n and Rudolph, Kara E},\n  title = {Beyond the Average Treatment Effect},\n  booktitle = {&lt;Insert conference&gt;},\n  year = {&lt;Insert year of confernece&gt;},\n  address = {&lt;Insert conference location&gt;},\n  month = {&lt;Insert conference month&gt;},\n  organization = {&lt;Insert conference organizer&gt;},\n  url = {https://www.beyondtheate.com}\n}",
    "crumbs": [
      "Instructors",
      "5. Final Remarks"
    ]
  },
  {
    "objectID": "R_ipsi.html",
    "href": "R_ipsi.html",
    "title": "Incremental Propensity Score Interventions",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]\nRecall that an incremental propensity score intervention (IPSI) is a hypothetical intervention where the conditional probability of treatment is shifted. We defined an IPSI, based on the risk ratio, that increased the likelihood of treatment as\n\\[\n\\dd_t(a_t, h_t, \\epsilon_t) = \\begin{cases}\na_t &\\text{ if } \\epsilon_t &lt; \\delta \\\\\n1 &\\text{ otherwise.}\n\\end{cases}\n\\]Assume we want to increase the likelihood of initiating treatment by 2-fold. We can implement this in R with",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "IPSI"
    ]
  },
  {
    "objectID": "R_ipsi.html#problem-1",
    "href": "R_ipsi.html#problem-1",
    "title": "Incremental Propensity Score Interventions",
    "section": "Problem 1",
    "text": "Problem 1\nWhat if we wanted to decrease the likelihood of initiating treatment by 2-fold? Implement this shift function in R. As a reminder\n\\[\n\\dd_t(a_t, h_t, \\epsilon_t) = \\begin{cases}\na_t &\\text{ if } \\epsilon_t &lt; \\delta \\\\\n0 &\\text{ otherwise.}\n\\end{cases}\n\\]\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\n\n✅ Solution\n\n\ndelta &lt;- 0.5\nd_ipsi_down &lt;- function(data, trt) {\n  eps &lt;- runif(nrow(data), 0, 1)\n  ifelse(eps &lt; delta, data[[trt]], 0)\n}\n\n\n\n\n\n\n\n\nlmtp already implements d_ipsi_up and d_ipsi_down as a single shift function factory, ipsi()!\n\nRisk ratio IPSIs that increase the likelihood of treatment should be specified with a value greater than 1 (i.e. a risk ratio IPSI that increases the likelihood of treatment 2-fold is equivalent to ipsi(2)).\nIPSIs that decrease the likelihood of treatment should be specified with a value less than 1 (i.e. a risk ratio IPSI that decreases the likelihood of treatment 2-fold is equivalent to ipsi(0.5))\n\n\n\n\nLet’s apply the shift function that increases the likelihood of initiating treatment by 2-fold to covid dataset.\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\n\nEstimating the effect of incremental propensity scores based on the risk ratio are easy to do with lmtp. Just use the ipsi() function as the input for the shift argument!",
    "crumbs": [
      "Instructors",
      "4. Estimating Effects with lmtp",
      "IPSI"
    ]
  },
  {
    "objectID": "info_tldr.html",
    "href": "info_tldr.html",
    "title": "The Causal Model and Notation",
    "section": "",
    "text": "\\[\n\\renewcommand{\\P}{\\mathsf{P}}\n\\newcommand{\\m}{\\mathsf{m}}\n\\newcommand{\\p}{\\mathsf{p}}\n\\newcommand{\\q}{\\mathsf{q}}\n\\newcommand{\\bb}{\\mathsf{b}}\n\\newcommand{\\g}{\\mathsf{g}}\n\\newcommand{\\rr}{\\mathsf{r}}\n\\newcommand{\\IF}{\\mathbb{IF}}\n\\newcommand{\\dd}{\\mathsf{d}}\n\\newcommand{\\Pn}{$\\mathsf{P}_n$}\n\\newcommand{\\E}{\\mathsf{E}}\n\\]",
    "crumbs": [
      "Instructors",
      "1. Introduction"
    ]
  },
  {
    "objectID": "info_tldr.html#notation",
    "href": "info_tldr.html#notation",
    "title": "The Causal Model and Notation",
    "section": "Notation",
    "text": "Notation\nConsider a typical setting where we have measured some treatment \\(A\\), some set of pre-treatment variables \\(L\\), and an outcome \\(Y\\). We will assume that the data are generated by the following causal model (but alternative definitions can be achieved using other causal models):\n\\[\n\\begin{align}\nL &= f_L(U_L)\\\\\nA &= f_A(L, U_A)\\\\\nY &= f_Y(A, L, U_Y)\n\\end{align}\n\\]\n\nThe functions \\(f\\) are assumed fixed, but likely unknown.\n\\(U = (U_L, U_A, U_Y)\\) is a vector of exogenous, unmeasured random variables.\nThis system is referred to as a Non-parametric Structural Equation Model (NPSEM).\nWe will assume that this system of equations exists in nature and is responsible for generating our observed data.\n\n\n\n\n\n\n\nWe will often refer to \\(L\\) as confounders, \\(A\\) as the treatment or exposure, and \\(Y\\) as the outcome.\n\n\n\nTake for example the following sample of data:\n\n\n\n\n\n\n\nLet, \\(L\\) = sex, bmi, age, and smoke; \\(A\\) = trt; and \\(Y\\) = event. When we say that we assume the previous NPSEM, we are positing that:\n\nthe variable trt was generated from an unknown function of the random variables sex, bmi, age, and smoke, as well as a set of unmeasured random variables not present in our data.\nthat the variable event was generated from an unknown function of the variables that generated trt in addition to trt itself.\n\nNote that in some cases, we may know one or more of the functions \\(f\\). For example, if our data came from a randomized trial for \\(A\\), then we know the function \\(f_A\\).",
    "crumbs": [
      "Instructors",
      "1. Introduction"
    ]
  },
  {
    "objectID": "info_tldr.html#counterfactuals",
    "href": "info_tldr.html#counterfactuals",
    "title": "The Causal Model and Notation",
    "section": "Counterfactuals",
    "text": "Counterfactuals\nCentral to how we will define causal effects is the concept of counterfactual random variables.\n\n\n\n\n\n\nCounterfactuals are random variables that would have been observed, possibly contrary to fact, in an alternative world.\n\n\n\nFor example, consider a scenario where we are interested in the value of \\(Y\\) in a hypothetical situation where, instead of the variable \\(A\\) being equal to its observed value, it is set to some other value \\(A^\\dd\\).\n\nThe value of \\(Y\\) in this hypothetical situation is a counterfactual random variable.\n\nTypically, one is interested in counterfactuals where treatment is set to some deterministic value. For instance, one could be interested in setting treatment \\(A^\\dd=1\\). One could also be interested in setting \\(A\\) according to some covariates, e.g., “treat if age &gt; 50”.\n\nThis intervention would be denoted as \\(A^\\dd=\\dd(L)\\) for some function \\(\\dd\\), and is known as a dynamic treatment regime.\n\nIn this workshop we are interested in a generalization of this concept, where the function \\(\\dd\\) can also depend on the natural value of treatment \\(A\\):\n\nLet \\(\\dd(A, L)\\) be a function that takes a natural treatment value \\(A\\) and a covariate profile \\(L\\) and returns a new value of treatment. We will refer to the function \\(\\dd\\) as a shift function or a general hypothetical intervention.\nDenote the value of \\(Y\\) in the hypothetical world where treatment is set to the value \\(\\dd(A,L)\\) as \\(Y^{\\dd}\\).\n\nReturning to the data example, imagine we are interested in the value of event if trt was replaced with the output of a function \\(\\dd\\) that always returns 1:\n\\[\n\\begin{align}\nL &= f_L(U_L)\\\\\nA^{\\dd} &= \\dd(A, L) = 1 \\\\\nY^{\\dd} &= f_Y(1, L, U_Y)\n\\end{align}\n\\]\nHere we introduce some new notation \\(A^{\\dd}\\) to refer to the post-intervention exposure. If we had the ability to collect data from this alternative NPSEM, the data may instead look like this:\n\n\n\n\n\n\n\nUnfortunately, we are never able to collect data from this alternative world. This is called the fundamental problem of causal inference.\nThe previous NPSEM is the simplest causal model we will assume in this workshop. However, real data is often much more complex and may be characterized by:\n\ntime-varying variables\nloss-to-follow-up\n\nAs such, we need to modify and introudce some additional notation:\n\n\n\n\n\n\n\nSymbol\nDefinition\n\n\n\n\n\\(i\\)\nThe index (i.e. a row in a dataset) of an observation from a data set with \\(n\\) total units (i.e., the total number of rows)\n\n\n\\(t\\)\nThe index of time for a total number of time points \\(\\tau\\)\n\n\n\\(L_t\\)\nConfounders at time \\(t\\)\n\n\n\\(A_t\\)\nA vector of intervention variables (i..e, treatment or exposure) at time \\(t\\)\n\n\n\\(Y\\)\nAn outcome variable observed at the end of the study, that is at time \\(\\tau + 1\\). Earlier measures of the outcome can be included in \\(L_t\\).\n\n\n\\(C_t\\)\nA indicator variable that a unit is observed (not censored) at time \\(t+1\\)\n\n\n\\(O_1, ..., O_n\\)\nA sample of \\(n\\) i.i.d observations with \\(O = (L_1, A_1, C_1, L_2, A_2, C_2, ..., L_\\tau, A_\\tau, C_\\tau, Y)\\)\n\n\n\\(\\bar{X}_t = (X_1, ..., X_t)\\)\nThe history of a variable up until time \\(t\\)\n\n\n\\(\\underline{X}_t = (X_t, ..., X_\\tau)\\)\nThe future of a variable, including time \\(t\\)\n\n\n\\(H_t = (\\bar{A}_{t-1}, \\bar{L}_t)\\)\nThe history of all variables up until just before \\(A_t\\)\n\n\n\\(\\epsilon_t\\)\nA randomizer\n\n\n\\(\\dd(a_t, h_t, \\epsilon_t)\\)\nA function that maps \\(A_t\\), \\(H_t\\), and \\(\\epsilon_t\\) to a new value of treatment \\(A^{\\dd}_t\\)",
    "crumbs": [
      "Instructors",
      "1. Introduction"
    ]
  }
]